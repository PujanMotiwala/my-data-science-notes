# Calculus

Calculus is the mathematics of continuous change. It provides tools for understanding rates of change (differentiation) and accumulation (integration), which are fundamental to optimizing machine learning models and working with continuous probability distributions.

This section covers:

*   **[[01_Functions_Limits_Continuity|Foundations]]**: Basic concepts of [[01_Functions_Limits_Continuity|Functions, Limits, and Continuity]], the definition and interpretation of [[02_Derivatives|Derivatives]], essential [[03_Rules_of_Differentiation|Differentiation Rules]], and the crucial [[04_Chain_Rule|Chain Rule]].
*   **[[01_Functions_of_Multiple_Variables|Multivariable Calculus]]**: Extending calculus to [[01_Functions_of_Multiple_Variables|Functions of Multiple Variables]], defining [[02_Partial_Derivatives|Partial Derivatives]], the [[03_Gradient|Gradient]] vector, and [[04_Directional_Derivatives|Directional Derivatives]].
*   **[[01_Maxima_and_Minima|Optimization]]**: Using calculus to find [[01_Maxima_and_Minima|Maxima and Minima]], the core algorithm of [[02_Gradient_Descent|Gradient Descent]] and its [[03_Gradient_Descent_Variants|variants]], and the important concept of [[04_Convexity|Convexity]].
*   **[[01_Hessian_Matrix|Advanced Topics]]**: Introduction to the [[01_Hessian_Matrix|Hessian Matrix]], the [[02_Jacobian_Matrix|Jacobian Matrix]], and [[03_Lagrange_Multipliers|Lagrange Multipliers]] for constrained optimization.
*   **[[01_Definite_and_Indefinite_Integrals|Integrals]]**: Understanding [[01_Definite_and_Indefinite_Integrals|Definite and Indefinite Integrals]] (Antiderivatives), the Fundamental Theorem of Calculus, and their direct [[02_Applications_in_Probability|Applications in Probability]] theory for continuous variables.
*   **[[01_Matrix_Calculus_Essentials|Matrix Calculus]]**: Combining linear algebra and calculus to differentiate with respect to vectors and matrices, essential for ML model training.

Use the sidebar navigation to explore specific topics within Calculus.