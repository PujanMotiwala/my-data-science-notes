# Linear Algebra

Linear Algebra is the branch of mathematics concerning [[02_Vectors|vector spaces]] and linear mappings between them. It provides the essential tools for representing and manipulating data, understanding geometric transformations, and solving systems of equations â€“ core operations in machine learning and data analysis.

This section includes:

*   **[[03_Matrices|Core Objects]]**: Definitions and properties of [[01_Scalars|Scalars]], [[02_Vectors|Vectors]], [[03_Matrices|Matrices]], and [[04_Tensors|Tensors]].
*   **[[02_Matrix_Operations|Basic Operations]]**: How to perform fundamental calculations like [[01_Vector_Operations|vector addition]], [[01_Vector_Operations|scalar multiplication]], [[01_Vector_Operations|dot products]], and [[02_Matrix_Operations|matrix multiplication]].
*   **[[03_Determinant|Matrix Properties]]**: Key concepts associated with matrices, such as the [[01_Identity_Matrix|Identity Matrix]], [[02_Inverse_Matrix|Matrix Inverse]], [[03_Determinant|Determinant]], and [[04_Trace|Trace]].
*   **[[02_Span_and_Basis|Vector Spaces]]**: Foundational ideas like [[01_Linear_Independence|Linear Independence]], [[02_Span_and_Basis|Span]], [[02_Span_and_Basis|Basis]], Dimension, and [[03_Rank|Rank]].
*   **[[02 Math/03 Linear Algebra/05 Norms/01_Definition|Norms]]**: Methods for measuring the size or length of vectors and matrices, including [[02_L1_Norm_Manhattan|L1]], [[03_L2_Norm_Euclidean|L2]], and [[04_Frobenius_Norm|Frobenius]] norms, essential for regularization and distance calculations.
*   **[[03_Overview|Decompositions]]**: Techniques for factoring matrices into simpler components, focusing on [[01_Eigenvalues_and_Eigenvectors|Eigenvalues and Eigenvectors]] and the [[02_Singular_Value_Decomposition_SVD|Singular Value Decomposition (SVD)]].

Use the sidebar navigation to explore specific topics within Linear Algebra.